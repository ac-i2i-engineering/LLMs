{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 RAG\n",
    "\n",
    "RAGs reference a knowledge base outside of its training data sources before generating a response. Because of this, RAGs extend the ability of LLMs to specific domains without the need of retraining. To create a RAG with Llama 3, we'll be using Huggingface. Make sure you have an account made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879e23bcf5a54a11a662dd1de3883c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "from textwrap import fill\n",
    "from langchain.prompts import PromptTemplate\n",
    "import locale\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Llama3\n",
    "This might need to be adjusted for your device. Come to us for consultation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88532ea536194ff3a29b1c942aeac8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.embed_positions': 0, 'model.layers': 0, 'model.norm': 0, 'lm_head': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Configuration for loading the model with CPU offloading\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit_fp32_cpu_offload=True)\n",
    "\n",
    "# Device mapping for model\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.embed_positions\": 0,\n",
    "    \"model.layers\": 0,\n",
    "    \"model.norm\": 0,\n",
    "    \"lm_head\": 0\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(model.hf_device_map)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Generation configuration\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
    "gen_cfg.max_new_tokens = 512\n",
    "gen_cfg.temperature = 0.0000001\n",
    "gen_cfg.return_full_text = True\n",
    "gen_cfg.do_sample = True\n",
    "gen_cfg.repetition_penalty = 1.11\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=gen_cfg\n",
    ")\n",
    "\n",
    "# Use the pipeline in HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 μs, sys: 0 ns, total: 287 μs\n",
      "Wall time: 269 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_template_llama3 = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "\n",
    "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template=prompt_template_llama3\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your documents\n",
    "Save the documents you want to fine tune on in your local repo. For this example, we'll use the Communist Manifesto, loaded in the content directory.\n",
    "\n",
    "Prior to running this cell, in your terminal, run:\n",
    "```bash\n",
    "conda install -c conda-forge poppler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw document...content/Communist_Manifesto.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='content/Wealth of Nations.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting text...\n",
      "Loading raw document...content/Wealth of Nations.pdf\n",
      "Splitting text...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3798"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "\n",
    "# add your PDF paths here\n",
    "loaders = [UnstructuredPDFLoader(fn) for fn in ['content/Communist_Manifesto.pdf',\n",
    "                                                'content/Wealth of Nations.pdf']]\n",
    "\n",
    "chunked_pdf_doc = []\n",
    "\n",
    "for loader in loaders:\n",
    "    print(\"Loading raw document...\" + loader.file_path)\n",
    "    pdf_doc = loader.load()\n",
    "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
    "    print(\"Splitting text...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
    "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
    "    chunked_pdf_doc.extend(documents)\n",
    "\n",
    "len(chunked_pdf_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 4s, sys: 618 ms, total: 9min 4s\n",
      "Wall time: 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# add and edit queries based on your documents\n",
    "query = \"Can a free market regulate itself?\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=200))\n",
    "\n",
    "print('#######################################################################')\n",
    "\n",
    "query = \"What would be the implications if free market principles were applied to a communist society?\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
